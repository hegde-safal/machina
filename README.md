# Machina â€” Intelligent Document Processing Platform

A web-based Intelligent Document Processing (IDP) solution designed for infrastructure and enterprise operations.  
Built during the **8th Mile x Overnight Hackathon**.

## ğŸš¨ Problem

Organizations handling engineering, HR, safety, procurement, and compliance documents struggle with:

- Unstructured formats (scanned PDFs, docs, images)
- No unified tracking or routing system
- Delayed decisions due to fragmented information
- Lost or duplicated documents
- Lack of traceability and institutional memory

## ğŸ¯ Solution

Machina automates document intake, understanding, routing, and search using:

- **OCR for text extraction** (pytesseract + tesseract-ocr)
- **Automated document processing** (Python backend)
- **Automatic storage & retrieval** (Supabase cloud database)
- **LLM-based summarization and classification** (coming soon)
- **Semantic search using embeddings** (coming soon)

---

## ğŸ“Š Architecture (Current Phase)

**Backend-Only Processing:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Backend (Deployed)           â”‚
â”‚                                     â”‚
â”‚  1. Query Supabase Database         â”‚
â”‚  2. Download from Storage Bucket    â”‚
â”‚  3. Run master_extractor.py         â”‚
â”‚  4. Save extracted text to DB       â”‚
â”‚  5. Update status                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Supabase (Cloud)                â”‚
â”‚                                     â”‚
â”‚  â”œâ”€ documents table (metadata)      â”‚
â”‚  â”œâ”€ Storage bucket (PDF files)      â”‚
â”‚  â””â”€ Extracted text (stored in DB)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Future Full Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Frontend        â”‚  Next.js  
â”‚  Login, Dashboards  â”‚
â”‚   Document Viewer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Backend        â”‚  FastAPI  
â”‚ Upload, Process, DB â”‚
â”‚  NLP + Routing API  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Intelligence Layer          â”‚
â”‚ OCR â†’ Summarization â†’ Classificationâ”‚
â”‚ Embeddings â†’ FAISS Semantic Search â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Storage & Persistence Layer     â”‚
â”‚ PostgreSQL (metadata + messages)   â”‚
â”‚ FAISS (vector search index)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§  Core Features

| Feature | Status |
|--------|--------|
| Role-based login (Admin vs Department Users) | â³ Future |
| Document upload (PDF / DOCX / Image) | âœ… |
| OCR + Text Extraction | âœ… |
| Auto summarization & routing | â³ Future |
| Priority & Due-date inference | â³ Future |
| Document status workflow | âœ… |
| Document-level chat | â³ Future |
| Semantic search with vector embeddings | â³ Future |
| Scalable architecture for enterprise use | âœ… |

## ğŸ“ Project Structure

```
machina/
â”œâ”€â”€ extraction/                    # Text extraction modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ master_extractor.py       # Main router for extraction
â”‚   â”œâ”€â”€ pdf.py                    # PDF text extraction
â”‚   â”œâ”€â”€ ocr.py                    # OCR for images & scanned PDFs
â”‚   â””â”€â”€ docx.py                   # DOCX extraction
â”œâ”€â”€ cleaning/
â”‚   â””â”€â”€ text_cleaner.py           # Text post-processing (placeholder)
â”œâ”€â”€ extracted_text/               # Output directory
â”‚   â”œâ”€â”€ IDP_Hackathon_Roadmap.txt
â”‚   â”œâ”€â”€ Intelligent_Document_Processing_Detailed_Roadmap.txt
â”‚   â””â”€â”€ PROBLEM_STATEMENTS.txt
â”œâ”€â”€ document_processor.py         # Main processing logic
â”œâ”€â”€ supabase_integration.py       # Supabase client wrapper
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ test_master_extractor.py      # Test extraction pipeline
â”œâ”€â”€ test_document_processor.py    # Test full workflow
â”œâ”€â”€ test_supabase_extraction.py   # Test Supabase integration
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. System Dependencies (for OCR)
```bash
# Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-eng

# macOS
brew install tesseract
```

### 3. Configure Supabase
Update `supabase_integration.py` or set environment variables:
```bash
export SUPABASE_URL="https://your-project.supabase.co"
export SUPABASE_SERVICE_ROLE_KEY="your-service-role-key"
```

### 4. Run Document Processing
```bash
python document_processor.py
```

## ğŸ“‹ Processing Pipeline

1. **Query Database** - Fetches documents with `status = 'Pending'` from Supabase
2. **Download Files** - Retrieves from Supabase Storage bucket to `/tmp`
3. **Extract Text** - Uses master_extractor with intelligent routing:
   - PDFs with text â†’ `pdfplumber` (fast digital extraction)
   - Scanned PDFs/Images â†’ `pytesseract` OCR (accurate for scanned docs)
   - DOCX files â†’ `docx2txt` extraction
4. **Save Results** - Stores extracted text in database
5. **Update Status** - Changes status to `'Text Extracted'`, sets `processed=True`, records timestamp

## ğŸ“Š Supported File Types

âœ… **PDF (digital text)** - Uses pdfplumber for fast extraction
âœ… **PDF (scanned)** - Uses pytesseract for OCR extraction
âœ… **DOCX** - Uses docx2txt for extraction
âœ… **Images (JPG, PNG)** - Uses pytesseract for OCR

## ï¿½ï¿½ Testing

```bash
# Test master extractor on sample PDF
python test_master_extractor.py

# Test full document processor
python test_document_processor.py

# Test Supabase integration
python test_supabase_extraction.py
```

## ğŸ“ˆ Performance Benchmarks

- Small PDF (< 5 pages): ~1-2 seconds
- Medium PDF (5-50 pages): ~5-10 seconds
- Large PDF (50+ pages): ~10-30 seconds
- Scanned PDF (OCR): ~10-60 seconds depending on size
- Batch (10 docs): ~30-120 seconds total

## ğŸŒ Deployment

### Railway
```bash
# Create Procfile
echo "worker: python document_processor.py" > Procfile

# Push to GitHub - Railway auto-deploys
git push origin main
```

### Your Backend / VPS
```bash
# Option 1: Run as cron job (every hour)
0 * * * * /usr/bin/python3 /path/to/document_processor.py

# Option 2: Run continuously
nohup python document_processor.py > processing.log 2>&1 &
```

## ğŸ§° Tech Stack

| Layer | Technology |
|-------|-----------|
| Backend Processing | Python 3.12 |
| PDF Extraction | pdfplumber |
| OCR | pytesseract + tesseract-ocr 5.3.4 |
| Document Formats | DOCX (docx2txt), Images (PIL/Pillow) |
| Cloud Database | Supabase (PostgreSQL) |
| Cloud Storage | Supabase Storage |
| Authentication | Supabase Auth + Service Role Key |

## ğŸ› ï¸ Core Modules

| File | Purpose |
|------|---------|
| `document_processor.py` | Main orchestrator (queries DB, downloads files, updates status) |
| `extraction/master_extractor.py` | File type router with intelligent fallback logic |
| `extraction/pdf.py` | Digital PDF text extraction using pdfplumber |
| `extraction/ocr.py` | OCR extraction for images & scanned PDFs |
| `extraction/docx.py` | DOCX extraction using docx2txt |
| `extraction/__init__.py` | Module exports and public API |
| `supabase_integration.py` | Supabase client wrapper (queries DB, downloads from storage) |
| `cleaning/text_cleaner.py` | Text post-processing (placeholder for future) |

## âœ… Current Status

**Fully Implemented:**
- âœ… PDF extraction (digital & scanned)
- âœ… OCR support for images and scanned PDFs
- âœ… DOCX support
- âœ… Supabase integration with Service Role Key
- âœ… Database queries and updates with timestamps
- âœ… Document status workflow
- âœ… File download & processing pipeline
- âœ… Batch document processing
- âœ… Error handling and logging

**In Development:**
- ğŸš§ Text cleaning pipeline
- ğŸš§ LLM summarization
- ğŸš§ Auto-categorization & routing
- ğŸš§ Priority & deadline detection
- ğŸš§ Frontend dashboard (Next.js)
- ğŸš§ Semantic search with embeddings
- ğŸš§ Document-level chat interface

## ğŸ“Š Database Schema

```sql
CREATE TABLE documents (
    id UUID PRIMARY KEY,
    original_filename TEXT NOT NULL,
    storage_path TEXT NOT NULL,
    status TEXT DEFAULT 'Pending',  -- 'Pending' or 'Text Extracted'
    extracted_text TEXT,
    processed BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸ‘¥ Team Machina

| Member | Role |
|--------|------|
| Safal Hegde | Backend + Database |
| Ryan Dave Fernandes | Frontend |
| Rohith S Panchamukhi | Machine Learning + OCR Pipeline |
| Stavan Rahul Khobare | UI/UX & Workflow Logic |

---

**Ready to process?** Run `python document_processor.py` ğŸš€
